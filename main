# -----------------------------
# Diabetes Prediction Template
# -----------------------------

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import os

os.makedirs("figures", exist_ok=True)


# -----------------------------
# Load dataset
# -----------------------------
# Replace with your file path
# Dataset link is in the README
df = pd.read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

# Inspect basic info
print(df.head())
print(df.info())
print(df.describe())

# -----------------------------
#  Split into train/test
# -----------------------------
X, y = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Diabetes_binary'])
print(f"Train shape: {X.shape}, Test shape: {y.shape}")

# -----------------------------
#  Identify numerical and categorical columns
# -----------------------------
target_col = 'Diabetes_binary'
feature_cols = [col for col in df.columns if col != target_col]

numerical_cols = X[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = X[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()

print("Numerical columns:", numerical_cols)
print("Categorical columns:", categorical_cols)

# -----------------------------
#  Exploratory Data Analysis
# -----------------------------
# Histogram of numerical features

num_features = len(numerical_cols)
cols = 5  # number of columns in the grid
rows = (num_features + cols - 1) // cols  # ceiling division to get enough rows

fig, axes = plt.subplots(rows, cols, figsize=(20, 4*rows), constrained_layout=True)

# flatten axes array to simplify indexing
axes = axes.flatten()

for i, col in enumerate(numerical_cols):
    sns.histplot(X[col], kde=True, bins=30, ax=axes[i])
    axes[i].set_title(col, fontsize=10)
    axes[i].set_xlabel("")
    axes[i].set_ylabel("")

# hide any extra subplots if number of features < rows*cols
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.show()

for i, col in enumerate(numerical_cols):
    plt.figure(figsize=(6,4))
    sns.boxplot(x=X[col])
    plt.title(f"Boxplot of {col}")
    plt.xlabel(col)
    plt.show()
#Boxplot may not be useful for this data due to its binary nature
# Correlation heatmap
plt.figure(figsize=(12, 10))
corr = X[numerical_cols + [target_col]].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.savefig("figures/model_comparison.png", dpi=300)
plt.show()

# -----------------------------
# Feature selection based on correlation
# -----------------------------
# Keep features with absolute correlation > 0.05 with target (common practice)
selected_features = corr[target_col][abs(corr[target_col]) > 0.05].index.tolist()
selected_features.remove(target_col)  # remove target itself
print("Selected features:", selected_features)

# -----------------------------
#  Preprocessing pipeline
# -----------------------------
# One-hot encode categorical features and scale numerical
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), selected_features),
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
])

# -----------------------------
#  Models
# -----------------------------
#checking for imbalance to reduce bias
positive = X[target_col].sum()
negative = len(X) - positive

ratio_positive_over_negative = positive / negative
ratio_negative_over_positive = negative / positive

scale_pos_weight = negative / positive
print("positive samples:", positive)
print("negative samples:", negative)

models = {
    "Logistic Regression": LogisticRegression(
        max_iter=1000,
        class_weight="balanced"
    ),
    "XGBoost Classifier": XGBClassifier(
        scale_pos_weight=scale_pos_weight,  # neg / pos
        eval_metric="logloss"
    )
}


# -----------------------------
# Train, evaluate and plot
# -----------------------------
results = {}
for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', model)])
    pipeline.fit(X, X[target_col])
    preds = pipeline.predict(y)
    acc = accuracy_score(y[target_col], preds)
    results[name] = acc
    print(f"\n{name} Accuracy: {acc:.4f}")
    print(f"{name} Classification Report:")
    print(classification_report(y[target_col], preds))
    
    # Confusion matrix
    cm = confusion_matrix(y[target_col], preds)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f"{name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.savefig(f"figures/{name.replace(' ', '_').lower()}_confusion_matrix.png", dpi=300)
    plt.show()

# -----------------------------
#  Compare model performance
# -----------------------------
plt.figure(figsize=(6,4))
ax = sns.barplot(
    x=list(results.keys()),
    y=list(results.values())
)

plt.ylabel("Accuracy")
plt.title("Model Comparison")
plt.ylim(0, 1)

# Add accuracy labels on top of bars
for p in ax.patches:
    height = p.get_height()
    ax.annotate(
        f"{height:.3f}",
        (p.get_x() + p.get_width() / 2., height),
        ha='center',
        va='bottom',
        fontsize=10,
        xytext=(0, 5),
        textcoords='offset points'
    )

plt.tight_layout()
plt.show()

